
input {
  file {
    path => "/local/hernandj/logstash-ingress/**/*.csv"
    start_position => beginning
    # to read from the beginning of file
    sincedb_path => "/dev/null"
  }
}

filter {
    csv {
        # Assume comma separator delimiter.
        separator => ","
        # Label each of the columns in the CSV file.
        columns => ["COMPLETE DESCRIPTION", "CATALOG PAGE", "PROMOTION TYPE", "MULTIBUY QTY", "PRICE", "DEPARTMENT/CATEGORY", "VALID START DATE", "ITEM TYPE", "DESCRIPTION", "DETAILS", "VALID END DATE", "PRICE RAW"]
    }
    # Skip over the header in the CSV file.
    if [COMPLETE DESCRIPTION] =~ /(?i)Complete Description/ or [COMPLETE DESCRIPTION] =~ /(?i)BLANK/ {
        drop { }
    }
    mutate {
        convert => {
            # Must tell Elastic search the types of the data.
            "CATALOG PAGE" => "integer"
            "MULTIBUY QTY" => "integer"
            "PRICE" => "float"
        }
    }
    date {
        match => ["VALID START DATE", "yyyy-MM-dd"]
    }
    date {
        match => ["VALID END DATE", "yyyy-MM-dd"]
    }
    # Strip out where this store item is from the path.
    grok {
        match => ["path", "/local/hernandj/logstash-ingress/(?<RETAILER>[^-]+)-(?<LOCATION>[^/]+)/"]
    }
    # Must use all lowercase for indexes.
    mutate {
        lowercase => [ "RETAILER", "LOCATION" ]
    }
}

output {
    stdout { codec => rubydebug }
    elasticsearch {
        # Assume Elasticsearch is running on localhost.
        hosts => ["localhost:9200"]
        # All input is loaded with this index.
        index => "catalytics-%{RETAILER}-%{LOCATION}-%{+YYYY-MM-dd}"
    }
}